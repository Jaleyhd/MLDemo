{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating music using RBM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this mini tutorial we will look into how to generate your own music. I would like to acknowledge Siraj Raval from youtube channel Sirajology for the awesome video.https://www.youtube.com/watch?v=ZE7qWXX05T0 . This is just a help guide for the code example shown by Siraj at his github repo https://github.com/llSourcell/Music_Generator_Demo\n",
    "\n",
    "Prerequisites for getting the code working is mentioned in github repo of Siraj :  https://github.com/llSourcell/Music_Generator_Demo, however I will state it again for ease :\n",
    "\n",
    "### Dependencies\n",
    "\n",
    "* Tensorflow\n",
    "* pandas\n",
    "* numpy\n",
    "* msgpack\n",
    "* glob\n",
    "* tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Midi\n",
    "Midi is a python Library used to generate sound from note-patterns. Look the bellow link for knowing more \n",
    "http://www.vikparuchuri.com/blog/making-instrumental-music-from-scratch/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "midi.Pattern(format=1, resolution=220, tracks=\\\n",
      "[midi.Track(\\\n",
      "  [midi.NoteOnEvent(tick=0, channel=0, data=[67, 40]),\n",
      "   midi.NoteOnEvent(tick=55, channel=0, data=[62, 40]),\n",
      "   midi.NoteOnEvent(tick=0, channel=0, data=[69, 40]),\n",
      "   midi.NoteOffEvent(tick=55, channel=0, data=[62, 0]),\n",
      "   midi.NoteOffEvent(tick=0, channel=0, data=[69, 0]),\n",
      "   midi.NoteOnEvent(tick=0, channel=0, data=[59, 40]),\n",
      "   midi.NoteOnEvent(tick=55, channel=0, data=[29, 40]),\n",
      "   midi.NoteOnEvent(tick=0, channel=0, data=[62, 40]),\n",
      "   midi.NoteOffEvent(tick=55, channel=0, data=[29, 0]),\n",
      "   midi.NoteOffEvent(tick=0, channel=0, data=[59, 0]),\n",
      "   midi.NoteOffEvent(tick=0, channel=0, data=[62, 0]),\n",
      "   midi.NoteOnEvent(tick=0, channel=0, data=[48, 40]),\n",
      "   midi.NoteOnEvent(tick=0, channel=0, data=[60, 40]),\n",
      "   midi.NoteOffEvent(tick=55, channel=0, data=[48, 0]),\n",
      "   midi.NoteOffEvent(tick=0, channel=0, data=[60, 0]),\n",
      "   midi.NoteOnEvent(tick=0, channel=0, data=[43, 40]),\n",
      "   midi.NoteOnEvent(tick=0, channel=0, data=[64, 40]),\n",
      "   midi.NoteOffEvent(tick=55, channel=0, data=[43, 0]),\n",
      "   midi.NoteOffEvent(tick=0, channel=0, data=[64, 0]),\n",
      "   midi.NoteOffEvent(tick=0, channel=0, data=[67, 0]),\n",
      "   midi.NoteOnEvent(tick=110, channel=0, data=[60, 40]),\n",
      "   midi.NoteOffEvent(tick=55, channel=0, data=[60, 0]),\n",
      "   midi.NoteOnEvent(tick=110, channel=0, data=[62, 40]),\n",
      "   midi.NoteOffEvent(tick=55, channel=0, data=[62, 0]),\n",
      "   midi.NoteOnEvent(tick=55, channel=0, data=[69, 40]),\n",
      "   midi.NoteOffEvent(tick=55, channel=0, data=[69, 0]),\n",
      "   midi.NoteOnEvent(tick=0, channel=0, data=[44, 40]),\n",
      "   midi.EndOfTrackEvent(tick=1, data=[])])])\n"
     ]
    }
   ],
   "source": [
    "import midi\n",
    "pattern = midi.read_midifile(\"sample.mid\")\n",
    "print pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "midi.Pattern(format=1, resolution=220, tracks=\\\n",
      "[midi.Track(\\\n",
      "  [midi.NoteOnEvent(tick=0, channel=0, data=[43, 20]),\n",
      "   midi.NoteOffEvent(tick=100, channel=0, data=[43, 0]),\n",
      "   midi.EndOfTrackEvent(tick=1, data=[])])])\n"
     ]
    }
   ],
   "source": [
    "import midi\n",
    "# Instantiate a MIDI Pattern (contains a list of tracks)\n",
    "pattern = midi.Pattern()\n",
    "# Instantiate a MIDI Track (contains a list of MIDI events)\n",
    "track = midi.Track()\n",
    "# Append the track to the pattern\n",
    "pattern.append(track)\n",
    "# Instantiate a MIDI note on event, append it to the track\n",
    "on = midi.NoteOnEvent(tick=0, velocity=20, pitch=midi.G_3)\n",
    "track.append(on)\n",
    "# Instantiate a MIDI note off event, append it to the track\n",
    "off = midi.NoteOffEvent(tick=100, pitch=midi.G_3)\n",
    "track.append(off)\n",
    "# Add the end of track event, append it to the track\n",
    "eot = midi.EndOfTrackEvent(tick=1)\n",
    "track.append(eot)\n",
    "# Print out the pattern\n",
    "print pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Understanding Serialization format MsgPack\n",
    "It is a more efficient serialization format for JSON like data. \n",
    "![alt text](msgpack.jpg \"MsgPack\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packed msg is   :  �\u0001\u0002\u0003\n",
      "unpacked msg is :  [1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "import msgpack\n",
    "packedmsg=msgpack.packb([1, 2, 3])\n",
    "unpackedmsg=msgpack.unpackb(packedmsg)\n",
    "print 'packed msg is   : ',packedmsg\n",
    "print 'unpacked msg is : ',unpackedmsg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Midi Songs in Msgpack serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126/126 [00:03<00:00, 33.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122 songs processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import midi\n",
    "import msgpack\n",
    "import midi_manipulation\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "import numpy as np\n",
    "\n",
    "def get_songs(path):\n",
    "    files = glob.glob('{}/*.mid*'.format(path))\n",
    "    songs = []\n",
    "    for f in tqdm(files):\n",
    "        try:\n",
    "            song = np.array(midi_manipulation.midiToNoteStateMatrix(f))\n",
    "            if np.array(song).shape[0] > 50:\n",
    "                songs.append(song)\n",
    "        except Exception as e:\n",
    "            raise e           \n",
    "    return songs\n",
    "\n",
    "songs = get_songs('Pop_Music_Midi') #These songs have already been converted from midi to msgpack\n",
    "print \"{} songs processed\".format(len(songs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Processed Songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(257, 156)\n",
      "(129, 156)\n",
      "(129, 156)\n",
      "(129, 156)\n",
      "(257, 156)\n"
     ]
    }
   ],
   "source": [
    "for idx,song in enumerate(songs):\n",
    "    song = np.array(song)\n",
    "    print song.shape\n",
    "    if(idx>3):break;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above codes shows the files of variable length encryped into a 156 dimentional vector unit. 156 is nothing but the twice the note range(2*(highest note - lowest note))  \n",
    "156=2(102-24)\n",
    "\n",
    "Now we need to use RBM to learn the note pattern.\n",
    "## RBM Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Credits : Siraj Raval, Sirajology Youtube Channel\n",
    "### HyperParameters\n",
    "# First, let's take a look at the hyperparameters of our model:\n",
    "\n",
    "lowest_note = midi_manipulation.lowerBound #the index of the lowest note on the piano roll\n",
    "highest_note = midi_manipulation.upperBound #the index of the highest note on the piano roll\n",
    "note_range = highest_note-lowest_note #the note range\n",
    "\n",
    "num_timesteps  = 15 #This is the number of timesteps that we will create at a time\n",
    "n_visible      = 2*note_range*num_timesteps #This is the size of the visible layer. \n",
    "n_hidden       = 50 #This is the size of the hidden layer\n",
    "\n",
    "num_epochs = 200 #The number of training epochs that we are going to run. For each epoch we go through the entire data set.\n",
    "batch_size = 100 #The number of training examples that we are going to send through the RBM at a time. \n",
    "lr         = tf.constant(0.005, tf.float32) #The learning rate of our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RBM Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x  : Shape of Data Variable       :  (?, 2340)\n",
      "W  : Shape of Weight Matrix       :  (2340, 50)\n",
      "bh : Shape of bias(hidden layer)  :  (1, 50)\n",
      "bv : Shape of bias(visible layer) :  (1, 2340)\n"
     ]
    }
   ],
   "source": [
    "x  = tf.placeholder(tf.float32, [None, n_visible], name=\"x\") #The placeholder variable that holds our data\n",
    "W  = tf.Variable(tf.random_normal([n_visible, n_hidden], 0.01), name=\"W\") #The weight matrix that stores the edge weights\n",
    "bh = tf.Variable(tf.zeros([1, n_hidden],  tf.float32, name=\"bh\")) #The bias vector for the hidden layer\n",
    "bv = tf.Variable(tf.zeros([1, n_visible],  tf.float32, name=\"bv\")) #The bias vector for the visible layer\n",
    "print 'x  : Shape of Data Variable       : ',x.get_shape()\n",
    "print 'W  : Shape of Weight Matrix       : ',W.get_shape()\n",
    "print 'bh : Shape of bias(hidden layer)  : ',bh.get_shape()\n",
    "print 'bv : Shape of bias(visible layer) : ',bv.get_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restricted Boltzman Machine Explaination\n",
    "\n",
    "The figure bellow shows RBM for the following example. Now What we will do is try to to train this RBM to mimic the existing tunes in an unsupervised fashion. In other words recreated input via hidden units should be as much close to input as possible. There is something called Gibbs energy which comes into play.\n",
    "\n",
    "$E(x,W)=-(a^Tv+b^Th+v^TWh)$\n",
    "\n",
    "**E**   :  Gibbs Free Energy  \n",
    "**a,b** :  bias weights(2340,50)  \n",
    "**W**   :  Weight Matrix linking hidden and visible units(2340x50)  \n",
    "**v**   :  visible unit vector(2340x1)  \n",
    "**h**   :  hidden unit vector(50x1)  \n",
    "\n",
    "Now this is called Gibbs free Energy which we need to minimize. By minimizing this energy we make sure we get better and better representation for input units, which means network is able to imitate input patterns.\n",
    "![\"Restricted Boltzman Machine\"](rbm.png \"Restricted Boltzman Machine\")\n",
    "\n",
    "Now what we are going to do, is Manipulate the song data and reshape it such that it becomes 15 timesteps at a time exposed to Visible unit. Again\n",
    "\n",
    "2340 = 156x15\n",
    "\n",
    "Now If song was (90x156) will become (6,2340) or each row will contain 15 notes. \n",
    "\n",
    "What we do is simple, update the state of Weight Matrix and biases based on recreation of input again.\n",
    "### Tensor Flow Variable Parameters Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This function lets us easily sample from a vector of probabilities\n",
    "def sample(probs):\n",
    "    #Takes in a vector of probabilities, and returns a random vector of 0s and 1s sampled from the input vector\n",
    "    return tf.floor(probs + tf.random_uniform(tf.shape(probs), 0, 1))\n",
    "\n",
    "#This function runs the gibbs chain. We will call this function in two places:\n",
    "#    - When we define the training update step\n",
    "#    - When we sample our music segments from the trained RBM\n",
    "def gibbs_sample(k):\n",
    "    #Runs a k-step gibbs chain to sample from the probability distribution of the RBM defined by W, bh, bv\n",
    "    def gibbs_step(count, k, xk):\n",
    "        #Runs a single gibbs step. The visible values are initialized to xk\n",
    "        hk = sample(tf.sigmoid(tf.matmul(xk, W) + bh)) #Propagate the visible values to sample the hidden values\n",
    "        xk = sample(tf.sigmoid(tf.matmul(hk, tf.transpose(W)) + bv)) #Propagate the hidden values to sample the visible values\n",
    "        return count+1, k, xk\n",
    "\n",
    "    #Run gibbs steps for k iterations\n",
    "    ct = tf.constant(0) #counter\n",
    "    [_, _, x_sample] = control_flow_ops.While(lambda count, num_iter, *args: count < num_iter,\n",
    "                                         gibbs_step, [ct, tf.constant(k), x], 1, False)\n",
    "    #This is not strictly necessary in this implementation, but if you want to adapt this code to use one of TensorFlow's\n",
    "    #optimizers, you need this in order to stop tensorflow from propagating gradients back through the gibbs step\n",
    "    x_sample = tf.stop_gradient(x_sample) \n",
    "    return x_sample\n",
    "### Training Update Code\n",
    "# Now we implement the contrastive divergence algorithm. First, we get the samples of x and h from the probability distribution\n",
    "#The sample of x\n",
    "x_sample = gibbs_sample(1) \n",
    "#The sample of the hidden nodes, starting from the visible state of x\n",
    "h = sample(tf.sigmoid(tf.matmul(x, W) + bh)) \n",
    "#The sample of the hidden nodes, starting from the visible state of x_sample\n",
    "h_sample = sample(tf.sigmoid(tf.matmul(x_sample, W) + bh)) \n",
    "\n",
    "#Next, we update the values of W, bh, and bv, based on the difference between the samples that we drew and the original values\n",
    "size_bt = tf.cast(tf.shape(x)[0], tf.float32)\n",
    "W_adder  = tf.mul(lr/size_bt, tf.sub(tf.matmul(tf.transpose(x), h), tf.matmul(tf.transpose(x_sample), h_sample)))\n",
    "bv_adder = tf.mul(lr/size_bt, tf.reduce_sum(tf.sub(x, x_sample), 0, True))\n",
    "bh_adder = tf.mul(lr/size_bt, tf.reduce_sum(tf.sub(h, h_sample), 0, True))\n",
    "#When we do sess.run(updt), TensorFlow will run all 3 update steps\n",
    "updt = [W.assign_add(W_adder), bv.assign_add(bv_adder), bh.assign_add(bh_adder)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Flow Training Session Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:15: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "100%|██████████| 200/200 [01:33<00:00,  2.34it/s]\n"
     ]
    }
   ],
   "source": [
    "### Run the graph!\n",
    "# Now it's time to start a session and run the graph! \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #First, we train the model\n",
    "    #initialize the variables of the model\n",
    "    init = tf.initialize_all_variables()\n",
    "    sess.run(init)\n",
    "    #Run through all of the training data num_epochs times\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        for song in songs:\n",
    "            #The songs are stored in a time x notes format. The size of each song is timesteps_in_song x 2*note_range\n",
    "            #Here we reshape the songs so that each training example is a vector with num_timesteps x 2*note_range elements\n",
    "            song = np.array(song)\n",
    "            song = song[:np.floor(song.shape[0]/num_timesteps)*num_timesteps]\n",
    "            song = np.reshape(song, [song.shape[0]/num_timesteps, song.shape[1]*num_timesteps])\n",
    "            #Train the RBM on batch_size examples at a time\n",
    "            for i in range(1, len(song), batch_size): \n",
    "                tr_x = song[i:i+batch_size]\n",
    "                sess.run(updt, feed_dict={x: tr_x})\n",
    "\n",
    "    #Now the model is fully trained, so let's make some music! \n",
    "    #Run a gibbs chain where the visible nodes are initialized to 0\n",
    "    sample = gibbs_sample(1).eval(session=sess, feed_dict={x: np.zeros((10, n_visible))})\n",
    "    for i in range(sample.shape[0]):\n",
    "        if not any(sample[i,:]):\n",
    "            continue\n",
    "        #Here we reshape the vector to be time x notes, and then save the vector as a midi file\n",
    "        S = np.reshape(sample[i,:], (num_timesteps, 2*note_range))\n",
    "        midi_manipulation.noteStateMatrixToMidi(S, \"generated_chord_{}\".format(i))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Rendering of Midi Notes\n",
    "Once we have trained The RBM Model, we need to extract synthetic set of Midi Notes. We start the back and fro generative process from zero vector of the size bellow\n",
    "\n",
    "$dim(xsampling)=10x2340$\n",
    "\n",
    "or in other words, we are initializing 10 mini musical songs of size 15 (15x156).\n",
    "\n",
    "That is we are doing gibbs sampling process only once to extract the generated notes and convert it back into midi notes. The RBM will reflect the projection of the weights upon the empty side of visible units. It will go back and forth once as k=1 in *gibbs_sample* function.\n",
    "\n",
    "Hurrey, so you will see 10 midi files in your folder which are generated via rbm. Try to play them using some midi player and enjoy the awesomeness of machine learning for synthetic music.  \n",
    "**Special thanks to Siraj Raval via Sirajology for sharing the code**\n",
    "\n",
    "# Full Code\n",
    "The full code of all the steps together put together is given bellow :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126/126 [00:04<00:00, 31.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122 songs processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:116: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "100%|██████████| 200/200 [01:30<00:00,  2.24it/s]\n"
     ]
    }
   ],
   "source": [
    "#This file is heavily based on Daniel Johnson's midi manipulation code in https://github.com/hexahedria/biaxial-rnn-music-composition\n",
    "# Code by : Siraj Raval,Sirajology\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import msgpack\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from tqdm import tqdm\n",
    "\n",
    "###################################################\n",
    "# In order for this code to work, you need to place this file in the same \n",
    "# directory as the midi_manipulation.py file and the Pop_Music_Midi directory\n",
    "\n",
    "import midi_manipulation\n",
    "\n",
    "def get_songs(path):\n",
    "    files = glob.glob('{}/*.mid*'.format(path))\n",
    "    songs = []\n",
    "    for f in tqdm(files):\n",
    "        try:\n",
    "            song = np.array(midi_manipulation.midiToNoteStateMatrix(f))\n",
    "            if np.array(song).shape[0] > 50:\n",
    "                songs.append(song)\n",
    "        except Exception as e:\n",
    "            raise e           \n",
    "    return songs\n",
    "\n",
    "songs = get_songs('Pop_Music_Midi') #These songs have already been converted from midi to msgpack\n",
    "print \"{} songs processed\".format(len(songs))\n",
    "###################################################\n",
    "\n",
    "### HyperParameters\n",
    "# First, let's take a look at the hyperparameters of our model:\n",
    "\n",
    "lowest_note = midi_manipulation.lowerBound #the index of the lowest note on the piano roll\n",
    "highest_note = midi_manipulation.upperBound #the index of the highest note on the piano roll\n",
    "note_range = highest_note-lowest_note #the note range\n",
    "\n",
    "num_timesteps  = 15 #This is the number of timesteps that we will create at a time\n",
    "n_visible      = 2*note_range*num_timesteps #This is the size of the visible layer. \n",
    "n_hidden       = 50 #This is the size of the hidden layer\n",
    "\n",
    "num_epochs = 200 #The number of training epochs that we are going to run. For each epoch we go through the entire data set.\n",
    "batch_size = 100 #The number of training examples that we are going to send through the RBM at a time. \n",
    "lr         = tf.constant(0.005, tf.float32) #The learning rate of our model\n",
    "\n",
    "### Variables:\n",
    "# Next, let's look at the variables we're going to use:\n",
    "\n",
    "x  = tf.placeholder(tf.float32, [None, n_visible], name=\"x\") #The placeholder variable that holds our data\n",
    "W  = tf.Variable(tf.random_normal([n_visible, n_hidden], 0.01), name=\"W\") #The weight matrix that stores the edge weights\n",
    "bh = tf.Variable(tf.zeros([1, n_hidden],  tf.float32, name=\"bh\")) #The bias vector for the hidden layer\n",
    "bv = tf.Variable(tf.zeros([1, n_visible],  tf.float32, name=\"bv\")) #The bias vector for the visible layer\n",
    "\n",
    "\n",
    "#### Helper functions. \n",
    "\n",
    "#This function lets us easily sample from a vector of probabilities\n",
    "def sample(probs):\n",
    "    #Takes in a vector of probabilities, and returns a random vector of 0s and 1s sampled from the input vector\n",
    "    return tf.floor(probs + tf.random_uniform(tf.shape(probs), 0, 1))\n",
    "\n",
    "#This function runs the gibbs chain. We will call this function in two places:\n",
    "#    - When we define the training update step\n",
    "#    - When we sample our music segments from the trained RBM\n",
    "def gibbs_sample(k):\n",
    "    #Runs a k-step gibbs chain to sample from the probability distribution of the RBM defined by W, bh, bv\n",
    "    def gibbs_step(count, k, xk):\n",
    "        #Runs a single gibbs step. The visible values are initialized to xk\n",
    "        hk = sample(tf.sigmoid(tf.matmul(xk, W) + bh)) #Propagate the visible values to sample the hidden values\n",
    "        xk = sample(tf.sigmoid(tf.matmul(hk, tf.transpose(W)) + bv)) #Propagate the hidden values to sample the visible values\n",
    "        return count+1, k, xk\n",
    "\n",
    "    #Run gibbs steps for k iterations\n",
    "    ct = tf.constant(0) #counter\n",
    "    [_, _, x_sample] = control_flow_ops.While(lambda count, num_iter, *args: count < num_iter,\n",
    "                                         gibbs_step, [ct, tf.constant(k), x], 1, False)\n",
    "    #This is not strictly necessary in this implementation, but if you want to adapt this code to use one of TensorFlow's\n",
    "    #optimizers, you need this in order to stop tensorflow from propagating gradients back through the gibbs step\n",
    "    x_sample = tf.stop_gradient(x_sample) \n",
    "    return x_sample\n",
    "\n",
    "### Training Update Code\n",
    "# Now we implement the contrastive divergence algorithm. First, we get the samples of x and h from the probability distribution\n",
    "#The sample of x\n",
    "x_sample = gibbs_sample(1) \n",
    "#The sample of the hidden nodes, starting from the visible state of x\n",
    "h = sample(tf.sigmoid(tf.matmul(x, W) + bh)) \n",
    "#The sample of the hidden nodes, starting from the visible state of x_sample\n",
    "h_sample = sample(tf.sigmoid(tf.matmul(x_sample, W) + bh)) \n",
    "\n",
    "#Next, we update the values of W, bh, and bv, based on the difference between the samples that we drew and the original values\n",
    "size_bt = tf.cast(tf.shape(x)[0], tf.float32)\n",
    "W_adder  = tf.mul(lr/size_bt, tf.sub(tf.matmul(tf.transpose(x), h), tf.matmul(tf.transpose(x_sample), h_sample)))\n",
    "bv_adder = tf.mul(lr/size_bt, tf.reduce_sum(tf.sub(x, x_sample), 0, True))\n",
    "bh_adder = tf.mul(lr/size_bt, tf.reduce_sum(tf.sub(h, h_sample), 0, True))\n",
    "#When we do sess.run(updt), TensorFlow will run all 3 update steps\n",
    "updt = [W.assign_add(W_adder), bv.assign_add(bv_adder), bh.assign_add(bh_adder)]\n",
    "\n",
    "\n",
    "### Run the graph!\n",
    "# Now it's time to start a session and run the graph! \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #First, we train the model\n",
    "    #initialize the variables of the model\n",
    "    init = tf.initialize_all_variables()\n",
    "    sess.run(init)\n",
    "    #Run through all of the training data num_epochs times\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        for song in songs:\n",
    "            #The songs are stored in a time x notes format. The size of each song is timesteps_in_song x 2*note_range\n",
    "            #Here we reshape the songs so that each training example is a vector with num_timesteps x 2*note_range elements\n",
    "            song = np.array(song)\n",
    "            song = song[:np.floor(song.shape[0]/num_timesteps)*num_timesteps]\n",
    "            song = np.reshape(song, [song.shape[0]/num_timesteps, song.shape[1]*num_timesteps])\n",
    "            #Train the RBM on batch_size examples at a time\n",
    "            for i in range(1, len(song), batch_size): \n",
    "                tr_x = song[i:i+batch_size]\n",
    "                sess.run(updt, feed_dict={x: tr_x})\n",
    "\n",
    "    #Now the model is fully trained, so let's make some music! \n",
    "    #Run a gibbs chain where the visible nodes are initialized to 0\n",
    "    sample = gibbs_sample(1).eval(session=sess, feed_dict={x: np.zeros((10, n_visible))})\n",
    "    for i in range(sample.shape[0]):\n",
    "        if not any(sample[i,:]):\n",
    "            continue\n",
    "        #Here we reshape the vector to be time x notes, and then save the vector as a midi file\n",
    "        S = np.reshape(sample[i,:], (num_timesteps, 2*note_range))\n",
    "        midi_manipulation.noteStateMatrixToMidi(S, \"generated_chord_{}\".format(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
