{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Basics\n",
    "\n",
    "NLTK is a widely used tools for preprocessing raw data. This tutorial will try to cover some of the basic contents of how NLTK can be useful tool for NLP tasks. For the purpose of this tutorial, I have taken references from https://github.com/zelandiya/KiwiPyCon-NLP-tutorial\n",
    "\n",
    "Credits : A Medelyan  \n",
    "\n",
    "## Dependencies\n",
    "* NLTK \n",
    "* movie_reviews corpus\n",
    "* punkt tokenizer model\n",
    "\n",
    "To install Punkt model, typing *nltk.download()* will open up the bellow GUI, where you can go to models, select punkt and click on download.\n",
    "![punkt_installer](punkt_install.jpg \"Punkt Installer\")\n",
    "\n",
    "\n",
    "# Dataset -1 : Movie Reviews\n",
    "\n",
    "We will use Movie review dataset for sake of understanding.\n",
    "\n",
    "### Downloading a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /home/jaley/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path where NLTK searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/jaley/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data']\n"
     ]
    }
   ],
   "source": [
    "print nltk.data.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the details of corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of documents in corpus  :  2000\n",
      "Categories of movie Review :  [u'neg', u'pos']\n",
      "\n",
      "Example of filenames(pos)  :  [u'pos/cv000_29590.txt', u'pos/cv001_18431.txt']\n",
      "Example of filenames(neg)  :  [u'neg/cv000_29416.txt', u'neg/cv001_19502.txt']\n",
      "\n",
      "862 Words from filenames(pos)  :  [u'films', u'adapted', u'from', u'comic', u'books', ...]\n",
      "\n",
      "879 Words from filenames(neg)  :  [u'plot', u':', u'two', u'teen', u'couples', u'go', ...]\n",
      "\n",
      "Example of raw Text is :  films adapted from comic books have had plenty of success , whether they're about superheroes ( batman , superman , spawn ) , or geared toward kids ( casper ) or the arthouse crowd ( ghost world ) , but there's never really been a comic book like from hell before \n",
      "\n",
      "Movie Review sentences :  [[u'films', u'adapted', u'from', u'comic', u'books', u'have', u'had', u'plenty', u'of', u'success', u',', u'whether', u'they', u\"'\", u're', u'about', u'superheroes', u'(', u'batman', u',', u'superman', u',', u'spawn', u')', u',', u'or', u'geared', u'toward', u'kids', u'(', u'casper', u')', u'or', u'the', u'arthouse', u'crowd', u'(', u'ghost', u'world', u')', u',', u'but', u'there', u\"'\", u's', u'never', u'really', u'been', u'a', u'comic', u'book', u'like', u'from', u'hell', u'before', u'.'], [u'for', u'starters', u',', u'it', u'was', u'created', u'by', u'alan', u'moore', u'(', u'and', u'eddie', u'campbell', u')', u',', u'who', u'brought', u'the', u'medium', u'to', u'a', u'whole', u'new', u'level', u'in', u'the', u'mid', u\"'\", u'80s', u'with', u'a', u'12', u'-', u'part', u'series', u'called', u'the', u'watchmen', u'.'], ...]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "print 'No of documents in corpus  : ',len(movie_reviews.fileids())\n",
    "print 'Categories of movie Review : ',movie_reviews.categories()\n",
    "print '\\nExample of filenames(pos)  : ',movie_reviews.fileids('pos')[:2]\n",
    "print 'Example of filenames(neg)  : ',movie_reviews.fileids('neg')[:2]\n",
    "\n",
    "#Words from sample files(pos)\n",
    "print '\\n{} Words from filenames(pos)  : '.format(len(movie_reviews.words('pos/cv000_29590.txt'))),\n",
    "print movie_reviews.words('pos/cv000_29590.txt')\n",
    "\n",
    "#Words from sample files(neg)\n",
    "print '\\n{} Words from filenames(neg)  : '.format(len(movie_reviews.words('neg/cv000_29416.txt'))),\n",
    "print movie_reviews.words('neg/cv000_29416.txt')\n",
    "print '\\nExample of raw Text is : ',movie_reviews.raw('pos/cv000_29590.txt').split('.')[0]\n",
    "print '\\nMovie Review sentences : ',movie_reviews.sents('pos/cv000_29590.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Most Frequent Words in Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent words in review\n",
      "[(u'all', 3), (u'childs', 1), (u'steve', 1), (u'surgical', 1), (u'comments', 1), (u'go', 1), (u'certainly', 1), (u'to', 15), (u'watchmen', 1), (u'song', 1), (u'very', 1), (u'simpsons', 1), (u'novel', 1), (u'jack', 2), (u'surgeon', 1), (u'level', 1), (u'did', 1), (u'turns', 2), (u'michael', 1), (u'flashy', 1)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "words = movie_reviews.words('pos/cv000_29590.txt')\n",
    "words_by_frequency = FreqDist(words)\n",
    "print 'Most frequent words in review'\n",
    "print words_by_frequency.items()[:20]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For most frequent words across both *pos* and *neg* categories, run the bellow code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Category neg\n",
      "[(u'sonja', 1), (u'askew', 4), (u'woods', 54), (u'spiders', 1), (u'bazooms', 1), (u'hanging', 37), (u'francesca', 3), (u'comically', 5), (u'disobeying', 1), (u'hennings', 2), (u'canet', 1), (u'originality', 34), (u'caned', 1), (u'rickman', 4), (u'stipulate', 1), (u'rawhide', 1), (u'bringing', 25), (u'unsworth', 1), (u'liaisons', 8), (u'wooden', 27)]\n",
      "Category pos\n",
      "[(u'woods', 36), (u'spiders', 3), (u'hanging', 22), (u'woody', 100), (u'comically', 7), (u'localized', 1), (u'scold', 2), (u'originality', 24), (u'mutinies', 1), (u'rickman', 11), (u'slothful', 1), (u'wracked', 1), (u'capoeira', 1), (u'rawhide', 1), (u'bringing', 56), (u'liaisons', 1), (u'grueling', 1), (u'sommerset', 4), (u'wooden', 21), (u'wednesday', 5)]\n"
     ]
    }
   ],
   "source": [
    "# Compare the most frequent words in both sets\n",
    "print ''\n",
    "for category in movie_reviews.categories():\n",
    "\n",
    "    print 'Category', category\n",
    "    all_words = movie_reviews.words(categories=category)\n",
    "    all_words_by_frequency = FreqDist(all_words)\n",
    "    print all_words_by_frequency.items()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stopwords\n",
    "Stopwords are redundent words in sentences which are encountered multiple times like a,the,from,etc.\n",
    "These includes articles,helping verbs,etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jaley/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Words with stopwords    :  [u'films', u'adapted', u'from', u'comic', u'books']\n",
      "Words without stopwords :  [u'films', u'adapted', u'comic', u'books', u'plenty']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from string import punctuation\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# Strip stopwords from text\n",
    "words = movie_reviews.words('pos/cv000_29590.txt')\n",
    "print 'Words with stopwords    : ',words[:5]\n",
    "no_stopwords = [word for word in words if word not in stop]\n",
    "print 'Words without stopwords : ',no_stopwords[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering by parts of speech\n",
    "In NLP parts of speech are tags assigned to each word in sentence based on the symmentic category it belongs to. This includes noun, pronoun, preposition,etc. It is not a linear, rather heirarchial. For example, a sentence can be split into Noun phrase,Verb Phrase,etc. Thereafter, it is further split into propernoun,etc. Look into Penn's Treebank and the explaination of POS Tags as given in youtube link bellow. \n",
    "https://www.youtube.com/watch?v=LivXkL2DO_w\n",
    "\n",
    "In the example bellow, we are using averaged perceptron tagger. Which means, it can be incorrect, as a lot of words have different parts of speech based on how it is used in sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jaley/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('sample', 'JJ'), ('text', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "print nltk.pos_tag('This is a sample text'.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here JJ is adjective, NN is noun singular and  NNS is noun plural.  \n",
    "Let us say we are interested in only adjectives, then we will do filtering based on POS as bellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjectives in list of words are  [u'comic', u'ghost', u'comic', u'whole', u'new', u'little', u'graphic', u'other', u'whole', u'comic', u'allen', u'ludicrous', u'violent', u'east', u'sooty', u'little', u'nervous', u'mysterious', u'surgical', u'first', u'robbie', u'johnny', u'prophetic', u'copious', u'mary', u'isn', u'gruesome', u'other', u'unique', u'interesting', u'comic', u'vertical', u'rafael', u'good', u'funny', u'capable', u'such', u'ghastly', u'electric', u'bleak', u'tim', u'victorian', u'flashy', u'crazy', u'twin', u'black', u'white', u'comic', u'original', u'solid', u'strong', u'british', u'great', u'big', u'graham', u'first', u'irish', u'bad', u'good', u'strong', u'suspect', u'critical', u'mtv', u'high', u'reese', u'current', u'simple', u'washington', u'high', u'student', u'reese', u'high', u'megalomaniac', u'popular']\n"
     ]
    }
   ],
   "source": [
    "all_words = movie_reviews.words(categories='pos')[:1000]\n",
    "pos_tagged = nltk.pos_tag(all_words)\n",
    "all_filtered_words = [x[0] for x in pos_tagged if x[1] in ('JJ') and len(x[0]) > 1]\n",
    "print 'Adjectives in list of words are ',all_filtered_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract NGrams and multi-word phrases\n",
    "When we have to predict a missing word, we have a few choices, either use previous word to predict it which is bigram model, to use previous two words which is trigram model.Now in bigram model, we look at pair of words to understand temporal structure of word with respect to sentence. Let us take examples to better understand the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test sentence      :  This is a sample sentence\n",
      "Tokenized sentence :  ['This', 'is', 'a', 'sample', 'sentence']\n",
      "Bigram pairs are   :  [('This', 'is'), ('is', 'a'), ('a', 'sample'), ('sample', 'sentence')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "sample_text='This is a sample sentence'\n",
    "print 'Test sentence      : ',sample_text\n",
    "tokenized_text=nltk.word_tokenize(sample_text)\n",
    "print 'Tokenized sentence : ',tokenized_text\n",
    "print 'Bigram pairs are   : ',[elem for elem in ngrams(tokenized_text,2)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Common BiGram\n",
    "We will use movies review dataset to find most common bigram/word pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 most common BiGrams are [(('Humpty', 'Dumpty'), 2), (('Dumpty', 'sat'), 1)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from nltk.probability import FreqDist\n",
    "test_sentence='Humpty Dumpty Humpty Dumpty sat on a wall'\n",
    "test_tokens=test_sentence.split(' ')\n",
    "print '2 most common BiGrams are',FreqDist(ngrams(test_tokens,2)).items()[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Common N-Grams\n",
    "Instead of looking at frequency of word pairs(bigram), we can also compare frequency of word phrases(word groups) of variable length. The bellow code gets list of all co-occuring word groups ranging from size 2 to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common N-Grams are  [('a wall', 1), ('on a', 1), ('Humpty Dumpty', 2)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.probability import FreqDist\n",
    "test_sentence='Humpty Dumpty Humpty Dumpty sat on a wall'\n",
    "test_tokens=test_sentence.split(' ')\n",
    "\n",
    "gramList=[' '.join(gram) for n in range(2,5) for gram in ngrams(test_tokens,n)]\n",
    "print 'Common N-Grams are ',FreqDist(gramList).items()[-3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Example\n",
    "It skips ngrams which start/end with stopword. It also ignores ngrams  that span punctuation boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of grams matching the proposed criteria are :  ['Humpty Dumpty', 'Dumpty Dumpty', 'Dumpty Humpty', 'Humpty Dumpty', 'Dumpty Dumpty', 'Dumpty Dumpty', 'Dumpty sat', 'Humpty Dumpty Dumpty', 'Dumpty Dumpty Humpty', 'Dumpty Humpty Dumpty', 'Humpty Dumpty Dumpty', 'Dumpty Dumpty Dumpty', 'Dumpty Dumpty sat', 'Humpty Dumpty Dumpty Humpty', 'Dumpty Dumpty Humpty Dumpty', 'Dumpty Humpty Dumpty Dumpty', 'Humpty Dumpty Dumpty Dumpty', 'Dumpty Dumpty Dumpty sat', 'sat on a wall']\n",
      "\n",
      " Common N-Grams are :  [('Dumpty Dumpty sat', 1), ('sat on a wall', 1), ('Dumpty sat', 1), ('Humpty Dumpty Dumpty Humpty', 1), ('Dumpty Humpty', 1), ('Dumpty Dumpty Humpty', 1), ('Dumpty Humpty Dumpty Dumpty', 1), ('Dumpty Dumpty Humpty Dumpty', 1), ('Dumpty Humpty Dumpty', 1), ('Humpty Dumpty Dumpty Dumpty', 1), ('Humpty Dumpty Dumpty', 2), ('Dumpty Dumpty Dumpty sat', 1), ('Dumpty Dumpty', 3), ('Dumpty Dumpty Dumpty', 1), ('Humpty Dumpty', 2)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "test_sentence='Humpty Dumpty Dumpty Humpty Dumpty Dumpty Dumpty sat on a wall'\n",
    "test_tokens=test_sentence.split(' ')\n",
    "\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "boundaries = ['(', ')', '.', ',', ';', ':']\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "def acceptable(word):\n",
    "    if word.lower() in stop:\n",
    "        return False\n",
    "    elif not word[0].isalpha():\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def has_no_boundaries(my_gram):\n",
    "    for my_word in my_gram:\n",
    "        if my_word in boundaries:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "gramList=[' '.join(gram) for n in range(2,5) for gram in ngrams(test_tokens,n) if acceptable(gram[-1]) and acceptable(gram[0]) and has_no_boundaries(gram) ]\n",
    "\n",
    "print 'List of grams matching the proposed criteria are : ', gramList\n",
    "\n",
    "print '\\n Common N-Grams are : ',FreqDist(gramList).items()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Scoring\n",
    "Tf–idf(Term frequency–inverse document frequency) is a measure to show word importance in a document. \n",
    "Mathematically it is written as \n",
    "\n",
    "$tfidf(w,d) = tf(w,d)\\times idf(w)$\n",
    "\n",
    "or in other words the score is product of term frequency and inverse document frequency. Please note that 'Term' and 'word' are same in current context. \n",
    "\n",
    "### Term Frequency\n",
    "\n",
    "$tf(w,d)=0.5+0.5\\times\\cfrac{f(w,d)}{max(f(w,d_i)}\\hspace{10mm}\\forall d_i\\in D$\n",
    "\n",
    "$f(w,d)$  = frequency of a word in document  \n",
    "$max(f(w,d_i))$ = maximum frequency of word  amongst all documents  \n",
    "\n",
    "Term Frequency is a floating point value between 0.5 to 1 and higher the value, higher the word frequency in docuement compare to all documents, higher the importance. For document with highest word frequency, it will be 1 and for document not containing the word, it will be 0.5.\n",
    "\n",
    "### Inverse Document Frequency\n",
    "\n",
    "$idf(w)=log(\\cfrac{N}{\\sum(I(w,d_i))}) \\hspace{10mm}\\forall d_i\\in D$\n",
    "\n",
    "$N$=Total number of documents\n",
    "$I(w,d_i)$=Indicator function stating if word is contained in document $d_i$\n",
    "$\\sum(I(w,d_i))$=Total number of documents containing the word\n",
    "\n",
    "Inverse document Frequency value states the importance of word, based on how frequently it is occouring. If it is very frequently occouring, than the word has lower importance, but if it is rare event the denominator decreases, which leads to increase in 'Inverse-Document frequency'.\n",
    "\n",
    "### Gensim Package\n",
    "Gensim package is a very popular python package for Topic Modelling. We will use it to find TF-IDF values.\n",
    "\n",
    "### Few Important Terms\n",
    "\n",
    "**Dictionary** : A unique mapping of a word to an integer value.  \n",
    "**BOW Document Format** : Bag of Words Document format stores a document as a list of words with its associated count.  \n",
    "So for *'Humpty Dumpty Humpty'*, it will be [(word2id('Humpty'),2),(word2id('Dumpty'),1)]\n",
    "where word2id is returning dictionary id of that word.  \n",
    "In above example, if dictionary is :  \n",
    "0 => Humpty\n",
    "1 => Dumpty  \n",
    "Then, BOW Document Format will return :  \n",
    "[(0,2),(1,1)]  \n",
    "[(word1Id,word1Count),(word2Id,word2Count) . . .]\n",
    "\n",
    "\n",
    "**Corpus** : Collection of Documents in BOW Format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Subset of First document in corpus according to BOW :  [(0, 6), (1, 1), (2, 1), (3, 2), (4, 1)]\n",
      "\n",
      "Subset of TFIDF value for words in first document :  [(0, 0.019263321498932638), (1, 0.044059324185709306), (2, 0.05605413673895676), (3, 0.02842430174654846), (4, 0.03518230106766029)]\n",
      "\n",
      " Above result with words instead of ids :  [(u'all', 0.019263321498932638), (u'concept', 0.044059324185709306), (u'skip', 0.05605413673895676), (u'go', 0.02842430174654846), (u'seemed', 0.03518230106766029)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "texts=[]\n",
    "#List of word lists. \n",
    "#[[doc1word1,doc1word2,doc1word3],[doc2],[doc3]]\n",
    "for fileid in movie_reviews.fileids():\n",
    "    words = movie_reviews.words(fileid)\n",
    "    texts.append(words)\n",
    "\n",
    "# Create a dictionary from list of documents\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# Create a Corpus based on BOW Format.\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "print '\\nSubset of First document in corpus according to BOW : ',corpus[0][:5]\n",
    "\n",
    "#Create a TFIDF Model for the corpus\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "\n",
    "print '\\nSubset of TFIDF value for words in first document : ',tfidf[corpus[0]][:5]\n",
    "print '\\n Above result with words instead of ids : ',[(dictionary.get(item[0]),item[1]) for item in tfidf[corpus[0]][:5]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
