{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Basics\n",
    "\n",
    "NLTK is a widely used tools for preprocessing raw data. This tutorial will try to cover some of the basic contents of how NLTK can be useful tool for NLP tasks. For the purpose of this tutorial, I have taken references from https://github.com/zelandiya/KiwiPyCon-NLP-tutorial\n",
    "\n",
    "## Dependencies\n",
    "* NLTK \n",
    "* movie_reviews corpus\n",
    "* punkt tokenizer model\n",
    "\n",
    "To install Punkt model, typing *nltk.download()* will open up the bellow GUI, where you can go to models, select punkt and click on download.\n",
    "![punkt_installer](punkt_install.jpg \"Punkt Installer\")\n",
    "\n",
    "\n",
    "# Dataset -1 : Movie Reviews\n",
    "\n",
    "We will use Movie review dataset for sake of understanding.\n",
    "\n",
    "### Downloading a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /home/jaley/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path where NLTK searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/jaley/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data']\n"
     ]
    }
   ],
   "source": [
    "print nltk.data.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the details of corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of documents in corpus  :  2000\n",
      "Categories of movie Review :  [u'neg', u'pos']\n",
      "\n",
      "Example of filenames(pos)  :  [u'pos/cv000_29590.txt', u'pos/cv001_18431.txt']\n",
      "Example of filenames(neg)  :  [u'neg/cv000_29416.txt', u'neg/cv001_19502.txt']\n",
      "\n",
      "862 Words from filenames(pos)  :  [u'films', u'adapted', u'from', u'comic', u'books', ...]\n",
      "\n",
      "879 Words from filenames(neg)  :  [u'plot', u':', u'two', u'teen', u'couples', u'go', ...]\n",
      "\n",
      "Example of raw Text is :  films adapted from comic books have had plenty of success , whether they're about superheroes ( batman , superman , spawn ) , or geared toward kids ( casper ) or the arthouse crowd ( ghost world ) , but there's never really been a comic book like from hell before \n",
      "\n",
      "Movie Review sentences :  [[u'films', u'adapted', u'from', u'comic', u'books', u'have', u'had', u'plenty', u'of', u'success', u',', u'whether', u'they', u\"'\", u're', u'about', u'superheroes', u'(', u'batman', u',', u'superman', u',', u'spawn', u')', u',', u'or', u'geared', u'toward', u'kids', u'(', u'casper', u')', u'or', u'the', u'arthouse', u'crowd', u'(', u'ghost', u'world', u')', u',', u'but', u'there', u\"'\", u's', u'never', u'really', u'been', u'a', u'comic', u'book', u'like', u'from', u'hell', u'before', u'.'], [u'for', u'starters', u',', u'it', u'was', u'created', u'by', u'alan', u'moore', u'(', u'and', u'eddie', u'campbell', u')', u',', u'who', u'brought', u'the', u'medium', u'to', u'a', u'whole', u'new', u'level', u'in', u'the', u'mid', u\"'\", u'80s', u'with', u'a', u'12', u'-', u'part', u'series', u'called', u'the', u'watchmen', u'.'], ...]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "print 'No of documents in corpus  : ',len(movie_reviews.fileids())\n",
    "print 'Categories of movie Review : ',movie_reviews.categories()\n",
    "print '\\nExample of filenames(pos)  : ',movie_reviews.fileids('pos')[:2]\n",
    "print 'Example of filenames(neg)  : ',movie_reviews.fileids('neg')[:2]\n",
    "\n",
    "#Words from sample files(pos)\n",
    "print '\\n{} Words from filenames(pos)  : '.format(len(movie_reviews.words('pos/cv000_29590.txt'))),\n",
    "print movie_reviews.words('pos/cv000_29590.txt')\n",
    "\n",
    "#Words from sample files(neg)\n",
    "print '\\n{} Words from filenames(neg)  : '.format(len(movie_reviews.words('neg/cv000_29416.txt'))),\n",
    "print movie_reviews.words('neg/cv000_29416.txt')\n",
    "print '\\nExample of raw Text is : ',movie_reviews.raw('pos/cv000_29590.txt').split('.')[0]\n",
    "print '\\nMovie Review sentences : ',movie_reviews.sents('pos/cv000_29590.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Most Frequent Words in Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent words in review\n",
      "[(u'all', 3), (u'childs', 1), (u'steve', 1), (u'surgical', 1), (u'comments', 1), (u'go', 1), (u'certainly', 1), (u'to', 15), (u'watchmen', 1), (u'song', 1), (u'very', 1), (u'simpsons', 1), (u'novel', 1), (u'jack', 2), (u'surgeon', 1), (u'level', 1), (u'did', 1), (u'turns', 2), (u'michael', 1), (u'flashy', 1)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "words = movie_reviews.words('pos/cv000_29590.txt')\n",
    "words_by_frequency = FreqDist(words)\n",
    "print 'Most frequent words in review'\n",
    "print words_by_frequency.items()[:20]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For most frequent words across both *pos* and *neg* categories, run the bellow code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Category neg\n",
      "[(u'sonja', 1), (u'askew', 4), (u'woods', 54), (u'spiders', 1), (u'bazooms', 1), (u'hanging', 37), (u'francesca', 3), (u'comically', 5), (u'disobeying', 1), (u'hennings', 2), (u'canet', 1), (u'originality', 34), (u'caned', 1), (u'rickman', 4), (u'stipulate', 1), (u'rawhide', 1), (u'bringing', 25), (u'unsworth', 1), (u'liaisons', 8), (u'wooden', 27)]\n",
      "Category pos\n",
      "[(u'woods', 36), (u'spiders', 3), (u'hanging', 22), (u'woody', 100), (u'comically', 7), (u'localized', 1), (u'scold', 2), (u'originality', 24), (u'mutinies', 1), (u'rickman', 11), (u'slothful', 1), (u'wracked', 1), (u'capoeira', 1), (u'rawhide', 1), (u'bringing', 56), (u'liaisons', 1), (u'grueling', 1), (u'sommerset', 4), (u'wooden', 21), (u'wednesday', 5)]\n"
     ]
    }
   ],
   "source": [
    "# Compare the most frequent words in both sets\n",
    "print ''\n",
    "for category in movie_reviews.categories():\n",
    "\n",
    "    print 'Category', category\n",
    "    all_words = movie_reviews.words(categories=category)\n",
    "    all_words_by_frequency = FreqDist(all_words)\n",
    "    print all_words_by_frequency.items()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stopwords\n",
    "Stopwords are redundent words in sentences which are encountered multiple times like a,the,from,etc.\n",
    "These includes articles,helping verbs,etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jaley/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Words with stopwords    :  [u'films', u'adapted', u'from', u'comic', u'books']\n",
      "Words without stopwords :  [u'films', u'adapted', u'comic', u'books', u'plenty']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from string import punctuation\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# Strip stopwords from text\n",
    "words = movie_reviews.words('pos/cv000_29590.txt')\n",
    "print 'Words with stopwords    : ',words[:5]\n",
    "no_stopwords = [word for word in words if word not in stop]\n",
    "print 'Words without stopwords : ',no_stopwords[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering by parts of speech\n",
    "In NLP parts of speech are tags assigned to each word in sentence based on the symmentic category it belongs to. This includes noun, pronoun, preposition,etc. It is not a linear, rather heirarchial. For example, a sentence can be split into Noun phrase,Verb Phrase,etc. Thereafter, it is further split into propernoun,etc. Look into Penn's Treebank and the explaination of POS Tags as given in youtube link bellow. \n",
    "https://www.youtube.com/watch?v=LivXkL2DO_w\n",
    "\n",
    "In the example bellow, we are using averaged perceptron tagger. Which means, it can be incorrect, as a lot of words have different parts of speech based on how it is used in sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jaley/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('sample', 'JJ'), ('text', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "print nltk.pos_tag('This is a sample text'.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here JJ is adjective, NN is noun singular and  NNS is noun plural.  \n",
    "Let us say we are interested in only adjectives, then we will do filtering based on POS as bellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjectives in list of words are  [u'comic', u'ghost', u'comic', u'whole', u'new', u'little', u'graphic', u'other', u'whole', u'comic', u'allen', u'ludicrous', u'violent', u'east', u'sooty', u'little', u'nervous', u'mysterious', u'surgical', u'first', u'robbie', u'johnny', u'prophetic', u'copious', u'mary', u'isn', u'gruesome', u'other', u'unique', u'interesting', u'comic', u'vertical', u'rafael', u'good', u'funny', u'capable', u'such', u'ghastly', u'electric', u'bleak', u'tim', u'victorian', u'flashy', u'crazy', u'twin', u'black', u'white', u'comic', u'original', u'solid', u'strong', u'british', u'great', u'big', u'graham', u'first', u'irish', u'bad', u'good', u'strong', u'suspect', u'critical', u'mtv', u'high', u'reese', u'current', u'simple', u'washington', u'high', u'student', u'reese', u'high', u'megalomaniac', u'popular']\n"
     ]
    }
   ],
   "source": [
    "all_words = movie_reviews.words(categories='pos')[:1000]\n",
    "pos_tagged = nltk.pos_tag(all_words)\n",
    "all_filtered_words = [x[0] for x in pos_tagged if x[1] in ('JJ') and len(x[0]) > 1]\n",
    "print 'Adjectives in list of words are ',all_filtered_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract NGrams and multi-word phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
