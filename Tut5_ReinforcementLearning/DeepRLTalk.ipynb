{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'starks': 0.11000000000000001, 'lannisters': 0.8900000000000001}\n",
      "{'starks': 0.18900000000000006, 'lannisters': 0.8110000000000002}\n",
      "{'starks': 0.18110000000000004, 'lannisters': 0.8189000000000002}\n",
      "{'starks': 0.18189000000000005, 'lannisters': 0.8181100000000002}\n",
      "{'starks': 0.18181100000000006, 'lannisters': 0.8181890000000003}\n",
      "{'starks': 0.18181100000000006, 'lannisters': 0.8181890000000003}\n",
      "Volla, 'lannisters' have been winners with 81.82% probability to be in king's landing \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# S: State Value of house : Its probability of retaining kings landing\n",
    "# P: Transition Matrix : from houseA to houseB, what is probability of transistion.\n",
    "\n",
    "houses = [\"lannisters\",\"starks\"]\n",
    "S = {\"lannisters\": 0.1, \"starks\":0.9}\n",
    "P = {\"from_lannisters\":{\"to_lannisters\": 0.8, \"to_starks\" : 0.2 }, \n",
    "     \"from_starks\": {\"to_starks\": 0.1, \"to_lannisters\": 0.9}}\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    lannisters_future_stateval, starks_future_stateval = 0,0;\n",
    "    \n",
    "    lannisters_future_stateval = S[\"lannisters\"]*P[\"from_lannisters\"][\"to_lannisters\"]+\\\n",
    "                                S[\"starks\"]*P[\"from_starks\"][\"to_lannisters\"];\n",
    "    \n",
    "    starks_future_stateval =  S[\"lannisters\"]*P[\"from_lannisters\"][\"to_starks\"]+\\\n",
    "                                S[\"starks\"]*P[\"from_starks\"][\"to_starks\"];\n",
    "\n",
    "    S[\"lannisters\"]=lannisters_future_stateval\n",
    "    S[\"starks\"] = starks_future_stateval\n",
    "    print S\n",
    "    \n",
    "print S\n",
    "print \"Volla, '%s' have been winners with %0.2f%% probability to be in king's landing \"%(max(S, key=S.get),100*max(S.values()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'whiteharbor': 6.085, 'winterfell': 58.5, 'dragonstone': -1.1107500000000001, 'dead-terminal': -50.0, 'alive-terminal': 100.0}\n"
     ]
    }
   ],
   "source": [
    "# Markov Reward Process Example\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "S ={\"dragonstone\": 1,\"whiteharbor\":1, \"winterfell\":1,\"alive-terminal\":-1,\"dead-terminal\":-1} # States\n",
    "R = {\"dragonstone\": 0,\"whiteharbor\":10, \"winterfell\":50,\"alive-terminal\":100,\"dead-terminal\":-50}\n",
    "P ={\"from_dragonstone\":{\"to_whiteharbor\":0.5,\"to_winterfell\":0.1,\"to_dead-terminal\":0.4},\\\n",
    "    \"from_whiteharbor\":{\"to_winterfell\":0.1,\"to_dead-terminal\":0.9},\n",
    "   \"from_winterfell\":{\"to_alive-terminal\":0.9,\"to_dead-terminal\":0.1},\n",
    "    \"from_alive-terminal\":{},\n",
    "    \"from_dead-terminal\":{}\n",
    "   }\n",
    "gamma = 0.1\n",
    "\n",
    "for i in range(100):\n",
    "    S1 = copy.copy(S);\n",
    "    for castle in S1.keys():\n",
    "        reward =0;\n",
    "        from_castle='from_'+castle\n",
    "        future_states = P[from_castle]\n",
    "        for to_castle in future_states:\n",
    "            reward=reward+P[from_castle][to_castle]*S[to_castle[3:]];\n",
    "        \n",
    "        S1[castle] = R[castle]+ gamma*(reward) \n",
    "    S = copy.copy(S1);\n",
    "print S\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Function {'whiteharbor': 11.51, 'winterfell': 58.5, 'dragonstone': 5.3075, 'dead-terminal': -50, 'alive-terminal': 100}\n",
      "Learned Policy {'from_whiteharbor': 'land', 'from_dragonstone': 'dragon', 'from_winterfell': 'land'}\n"
     ]
    }
   ],
   "source": [
    "# Markov Decision Process Example\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "S ={\"dragonstone\": 1,\"whiteharbor\":1, \"winterfell\":1,\"alive-terminal\":-1,\"dead-terminal\":-1} # States\n",
    "\n",
    "R = {\"dragonstone\": 0,\"whiteharbor\":10, \"winterfell\":50,\"alive-terminal\":100,\"dead-terminal\":-50}\n",
    "P ={\"from_dragonstone\":{\"land\":{\"to_winterfell\":0.5,\"to_dead-terminal\":0.5},\\\n",
    "                         \"sea\":{\"to_whiteharbor\":0.1,\"to_dead-terminal\":0.9},\\\n",
    "                         \"dragon\":{\"to_winterfell\":0.95,\"to_dead-terminal\":0.05}},\\\n",
    "    \"from_whiteharbor\":{\"land\":{\"to_winterfell\":0.6,\"to_dead-terminal\":0.4}},\\\n",
    "   \"from_winterfell\":{\"land\":{\"to_alive-terminal\":0.9,\"to_dead-terminal\":0.1}},\\\n",
    "    \"from_alive-terminal\":{},\\\n",
    "    \"from_dead-terminal\":{}\n",
    "   }\n",
    "\n",
    "gamma = 0.1\n",
    "\n",
    "Policy = {\"from_dragonstone\":\"land\",\"from_whiteharbor\":\"land\",\"from_winterfell\":\"land\"}\n",
    "\n",
    "\n",
    "# Solution by Value Iteration\n",
    "for i in range(1000):\n",
    "    S1 = copy.copy(S);\n",
    "    \n",
    "    for castle in S1.keys():\n",
    "        from_castle='from_'+castle\n",
    "        future_states = P[from_castle]\n",
    "        action_reward={}\n",
    "        for action in future_states.keys():\n",
    "            reward =0;\n",
    "            for to_castle in future_states[action]:\n",
    "                reward=reward+P[from_castle][action][to_castle]*S[to_castle[3:]];\n",
    "            action_reward[action]=reward\n",
    "        if(len(action_reward.keys())>0): \n",
    "            Policy[from_castle]=max(action_reward,key=action_reward.get)\n",
    "            S1[castle] = R[castle]+gamma*(max(action_reward.values()))\n",
    "        else:\n",
    "            S1[castle] = R[castle]\n",
    "    S = copy.copy(S1);\n",
    "print 'Value Function',S\n",
    "print 'Learned Policy',Policy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Solution of MDP by Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Function {'whiteharbor': 11.51, 'winterfell': 58.5, 'dragonstone': 5.3075, 'dead-terminal': -50, 'alive-terminal': 100}\n",
      "Learned Policy {'from_whiteharbor': 'land', 'from_dragonstone': 'dragon', 'from_winterfell': 'land'}\n"
     ]
    }
   ],
   "source": [
    "#Solution by Policy Iteration    \n",
    "Policy = {\"from_dragonstone\":\"land\",\"from_whiteharbor\":\"land\",\"from_winterfell\":\"land\"}\n",
    "\n",
    "for i in range(10):\n",
    "    #Step 1 : Update Value Fuction\n",
    "    for j in range(100):\n",
    "        S1 = copy.copy(S);\n",
    "        for castle in S1.keys():\n",
    "            from_castle='from_'+castle\n",
    "            future_states = P[from_castle]\n",
    "            action_reward={}\n",
    "            if (from_castle not in Policy.keys()):\n",
    "                S1[castle]=R[castle]\n",
    "                continue\n",
    "            action = Policy[from_castle] #Based on \n",
    "            reward=0;\n",
    "            for to_castle in future_states[action]:\n",
    "                reward=reward+P[from_castle][action][to_castle]*S[to_castle[3:]];\n",
    "            S1[castle] = R[castle]+ gamma*(reward)            \n",
    "        S = copy.copy(S1);\n",
    "    #Step 2 : Update Policy\n",
    "    for castle in S.keys():\n",
    "        from_castle='from_'+castle\n",
    "        future_states = P[from_castle]\n",
    "        action_reward={}\n",
    "        for action in future_states.keys():\n",
    "            reward =0;\n",
    "            for to_castle in future_states[action]:\n",
    "                reward=reward+P[from_castle][action][to_castle]*S[to_castle[3:]];\n",
    "            action_reward[action]=reward\n",
    "        if(len(action_reward.keys())>0):\n",
    "            Policy[from_castle]=max(action_reward,key=action_reward.get) #Update Policy\n",
    "\n",
    "    \n",
    "print 'Value Function',S\n",
    "print 'Learned Policy',Policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
